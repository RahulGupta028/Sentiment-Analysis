{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7f1781",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa5be20",
   "metadata": {},
   "source": [
    "Fine-Grained Sentiment:\n",
    "This type of analysis gives you an understanding of customer feedback, get the precise result in the terms of the polarity of the input. \n",
    "Review labels are - Postive , negative , very positive , very negative and neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d94a89",
   "metadata": {},
   "source": [
    "The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.\n",
    "\n",
    "train.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.\n",
    "test.tsv contains just phrases. You must assign a sentiment label to each phrase.\n",
    "The sentiment labels are:\n",
    "\n",
    "0 - negative\n",
    "1 - somewhat negative\n",
    "2 - neutral\n",
    "3 - somewhat positive\n",
    "4 - positive\n",
    "\n",
    "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "889fefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string # from some string manipulation tasks\n",
    "import nltk # natural language toolkit\n",
    "import re # regex\n",
    "from string import punctuation # solving punctuation problems\n",
    "from nltk.corpus import stopwords # stop words in sentences\n",
    "from nltk.stem import WordNetLemmatizer # For stemming the sentence\n",
    "from nltk.stem import SnowballStemmer # For stemming the sentence\n",
    "from contractions import contractions_dict # to solve contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7716c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploting library \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pylab as pl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42870737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#train_data = pd.read_csv('train.tsv/train.tsv', delimiter='\\t', index_col = 'PhraseId')\n",
    "train_data = pd.read_csv(\"train.tsv/train.tsv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41474289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d825aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72721931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      "PhraseId      156060 non-null int64\n",
      "SentenceId    156060 non-null int64\n",
      "Phrase        156060 non-null object\n",
      "Sentiment     156060 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5b3ce5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output is Sentiment i.e  negative=0 , somewhat negative=1 ,neutral=2 ,somewhat positive=3 ,positive = 4\n",
    "train_data[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6ce2d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data[\"SentenceId\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61b36890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using NLTK library easy implement Tokenization\n",
    "#take string input and return a list of sentences, use nltk.sent_tokenize() to split the sentences.\n",
    "def sent_tokenize(text):\n",
    "    return nltk.sent_tokenize(text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2f50648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function return the list of the words\n",
    "def word_tokenize(text):\n",
    "    return nltk.word_tokenize(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba168378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [A series of escapades demonstrating the adage...\n",
       "1         [A series of escapades demonstrating the adage...\n",
       "2                                                [A series]\n",
       "3                                                       [A]\n",
       "4                                                  [series]\n",
       "5         [of escapades demonstrating the adage that wha...\n",
       "6                                                      [of]\n",
       "7         [escapades demonstrating the adage that what i...\n",
       "8                                               [escapades]\n",
       "9         [demonstrating the adage that what is good for...\n",
       "10                                [demonstrating the adage]\n",
       "11                                          [demonstrating]\n",
       "12                                              [the adage]\n",
       "13                                                    [the]\n",
       "14                                                  [adage]\n",
       "15                        [that what is good for the goose]\n",
       "16                                                   [that]\n",
       "17                             [what is good for the goose]\n",
       "18                                                   [what]\n",
       "19                                  [is good for the goose]\n",
       "20                                                     [is]\n",
       "21                                     [good for the goose]\n",
       "22                                                   [good]\n",
       "23                                          [for the goose]\n",
       "24                                                    [for]\n",
       "25                                              [the goose]\n",
       "26                                                  [goose]\n",
       "27        [is also good for the gander , some of which o...\n",
       "28        [is also good for the gander , some of which o...\n",
       "29                                                [is also]\n",
       "                                ...                        \n",
       "156030                        [a joke in the United States]\n",
       "156031    [The movie 's downfall is to substitute plot f...\n",
       "156032                              [The movie 's downfall]\n",
       "156033            [is to substitute plot for personality .]\n",
       "156034              [is to substitute plot for personality]\n",
       "156035                 [to substitute plot for personality]\n",
       "156036                    [substitute plot for personality]\n",
       "156037                                    [substitute plot]\n",
       "156038                                    [for personality]\n",
       "156039    [The film is darkly atmospheric , with Herrman...\n",
       "156040    [is darkly atmospheric , with Herrmann quietly...\n",
       "156041    [is darkly atmospheric , with Herrmann quietly...\n",
       "156042                            [is darkly atmospheric ,]\n",
       "156043                              [is darkly atmospheric]\n",
       "156044    [with Herrmann quietly suggesting the sadness ...\n",
       "156045    [Herrmann quietly suggesting the sadness and o...\n",
       "156046                                           [Herrmann]\n",
       "156047    [quietly suggesting the sadness and obsession ...\n",
       "156048    [suggesting the sadness and obsession beneath ...\n",
       "156049               [suggesting the sadness and obsession]\n",
       "156050                          [the sadness and obsession]\n",
       "156051                              [sadness and obsession]\n",
       "156052                                        [sadness and]\n",
       "156053        [beneath Hearst 's forced avuncular chortles]\n",
       "156054                [Hearst 's forced avuncular chortles]\n",
       "156055                                          [Hearst 's]\n",
       "156056                          [forced avuncular chortles]\n",
       "156057                                 [avuncular chortles]\n",
       "156058                                          [avuncular]\n",
       "156059                                           [chortles]\n",
       "Name: Phrase, Length: 156060, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying sent_tokenize in phrase\n",
    "train_data['Phrase'].apply(sent_tokenize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7ec9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we convert all the works or sentence in lower case, then model will perform well \n",
    "\n",
    "def convert_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e2a6d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         a series of escapades demonstrating the adage ...\n",
       "1         a series of escapades demonstrating the adage ...\n",
       "2                                                  a series\n",
       "3                                                         a\n",
       "4                                                    series\n",
       "5         of escapades demonstrating the adage that what...\n",
       "6                                                        of\n",
       "7         escapades demonstrating the adage that what is...\n",
       "8                                                 escapades\n",
       "9         demonstrating the adage that what is good for ...\n",
       "10                                  demonstrating the adage\n",
       "11                                            demonstrating\n",
       "12                                                the adage\n",
       "13                                                      the\n",
       "14                                                    adage\n",
       "15                          that what is good for the goose\n",
       "16                                                     that\n",
       "17                               what is good for the goose\n",
       "18                                                     what\n",
       "19                                    is good for the goose\n",
       "20                                                       is\n",
       "21                                       good for the goose\n",
       "22                                                     good\n",
       "23                                            for the goose\n",
       "24                                                      for\n",
       "25                                                the goose\n",
       "26                                                    goose\n",
       "27        is also good for the gander , some of which oc...\n",
       "28        is also good for the gander , some of which oc...\n",
       "29                                                  is also\n",
       "                                ...                        \n",
       "156030                          a joke in the united states\n",
       "156031    the movie 's downfall is to substitute plot fo...\n",
       "156032                                the movie 's downfall\n",
       "156033              is to substitute plot for personality .\n",
       "156034                is to substitute plot for personality\n",
       "156035                   to substitute plot for personality\n",
       "156036                      substitute plot for personality\n",
       "156037                                      substitute plot\n",
       "156038                                      for personality\n",
       "156039    the film is darkly atmospheric , with herrmann...\n",
       "156040    is darkly atmospheric , with herrmann quietly ...\n",
       "156041    is darkly atmospheric , with herrmann quietly ...\n",
       "156042                              is darkly atmospheric ,\n",
       "156043                                is darkly atmospheric\n",
       "156044    with herrmann quietly suggesting the sadness a...\n",
       "156045    herrmann quietly suggesting the sadness and ob...\n",
       "156046                                             herrmann\n",
       "156047    quietly suggesting the sadness and obsession b...\n",
       "156048    suggesting the sadness and obsession beneath h...\n",
       "156049                 suggesting the sadness and obsession\n",
       "156050                            the sadness and obsession\n",
       "156051                                sadness and obsession\n",
       "156052                                          sadness and\n",
       "156053          beneath hearst 's forced avuncular chortles\n",
       "156054                  hearst 's forced avuncular chortles\n",
       "156055                                            hearst 's\n",
       "156056                            forced avuncular chortles\n",
       "156057                                   avuncular chortles\n",
       "156058                                            avuncular\n",
       "156059                                             chortles\n",
       "Name: Phrase, Length: 156060, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Phrase'].apply(convert_lowercase)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9267861",
   "metadata": {},
   "source": [
    "#make then model more accurate need to do preprocessing , need to check the spell as well\n",
    "# so using the speller module we can autocorrect the spelling of words\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "def autocorrect_spells(text):\n",
    "    correctspell= [spell(words) for words in (nltk.word_tokenize(text))]\n",
    "    return \" \".join(correctspell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7585d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data['Phrase'][:200].apply(autocorrect_spells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c88912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take string input and return a clean text without numbers.Use regex to discard the numbers.\n",
    "def remove_num(text):\n",
    "    output = ''.join(c for c in text if not c.isdigit())\n",
    "    return output \n",
    "\n",
    "def remove_punct(text):\n",
    "    return ''.join(c for c in text if c not in punctuation) \n",
    "\n",
    "#removes all the stop words like \"is,the,a,...\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return ' '.join([w for w in nltk.word_tokenize(sentence) if not w in stop_words]) \n",
    "\n",
    "def lemmatize(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_word = [wordnet_lemmatizer.lemmatize(word)for word in nltk.word_tokenize(text)]\n",
    "    return \" \".join(lemmatized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6f8ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    lower_text = convert_lowercase(text)\n",
    "    sentence_tokens = sent_tokenize(lower_text)\n",
    "    word_list = []\n",
    "    for each_sent in sentence_tokens:\n",
    "        lemmatizzed_sent = lemmatize(each_sent)\n",
    "        clean_text = remove_num(lemmatizzed_sent)\n",
    "        clean_text = remove_punct(clean_text)\n",
    "        clean_text = remove_stopwords(clean_text)\n",
    "        word_tokens = word_tokenize(clean_text)\n",
    "        for i in word_tokens:\n",
    "            word_list.append(i)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17bd77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a46bcecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer=preprocess) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3dac9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fab4fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4142bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', cv),  # strings to token integer counts\n",
    "    ('tfidf', tfidf),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', classifier),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aaee40ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bow', CountVectorizer(analyzer=<function preprocess at 0x000002C0512F1948>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=...f=False, use_idf=True)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(train_data['Phrase'],train_data['Sentiment']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc2b6034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.08      0.15      7072\n",
      "           1       0.61      0.34      0.43     27273\n",
      "           2       0.64      0.92      0.76     79582\n",
      "           3       0.60      0.45      0.51     32927\n",
      "           4       0.78      0.10      0.18      9206\n",
      "\n",
      "   micro avg       0.63      0.63      0.63    156060\n",
      "   macro avg       0.68      0.38      0.41    156060\n",
      "weighted avg       0.64      0.63      0.59    156060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "all_predictions = pipeline.predict(train_data['Phrase'])\n",
    "print(classification_report(train_data['Sentiment'], all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45679c53",
   "metadata": {},
   "source": [
    "NLTK is used to clean the data and preprocess it. Another tool that can be used is known as SpaCy. SpaCy is a library designed for fast and practical work, to avoid wasting time on NLP projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f51c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d8f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenizer_spacy(my_doc):\n",
    "    tokenizer = Tokenizer(nlp.vocab)\n",
    "    return tokenizer(my_doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46507e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "def filter_words(my_doc):\n",
    "    filtered_sent=[]\n",
    "    for word in my_doc:\n",
    "        if word.is_stop==False:\n",
    "            filtered_sent.append(word)\n",
    "    #print(\"Filtered Sentence:\",filtered_sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_spcay(my_doc):\n",
    "    lem_word = []\n",
    "    for i in my_doc:\n",
    "        lem_word.append(i.lemma_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partofSpeach(my_doc):\n",
    "    for word in my_doc:\n",
    "        print(word.text,word.pos_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunch(my_doc):\n",
    "    nopunc = []\n",
    "    for word in my_doc:\n",
    "        if word.pos_ != 'PUNCT':\n",
    "            nopunc.append(word)\n",
    "    #print(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b3c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stopwards = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "\n",
    "def preprocessText(text):\n",
    "    \n",
    "    noNum = \"\".join([i for i in text if not i.isdigit()])\n",
    "    \n",
    "    tokenize_list = nlp(noNum)\n",
    "    \n",
    "    tokenize_list = [word.lemma_.lower().strip() if word.lemma_ !=\"-PRON-\" else word.lower_ for word in tokenized_list]\n",
    "    \n",
    "    tokenize_list = [word for word in tokenize_list if word not in punctuations]\n",
    "    \n",
    "    return tokenized_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "51d7b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "81b71730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...True, vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = preprocess)\n",
    "# Create pipeline \n",
    "pipe = Pipeline([('tfidf',tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(train_data['Phrase'],train_data['Sentiment']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16c591b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.08      0.15      7072\n",
      "           1       0.61      0.34      0.43     27273\n",
      "           2       0.64      0.92      0.76     79582\n",
      "           3       0.60      0.45      0.51     32927\n",
      "           4       0.78      0.10      0.18      9206\n",
      "\n",
      "   micro avg       0.63      0.63      0.63    156060\n",
      "   macro avg       0.68      0.38      0.41    156060\n",
      "weighted avg       0.64      0.63      0.59    156060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe.score(train_data['Phrase'],train_data['Sentiment']) \n",
    "from sklearn.metrics import classification_report\n",
    "Pred = pipe.predict(train_data['Phrase'])\n",
    "print(classification_report(train_data['Sentiment'], Pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "830950b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.tsv/test.tsv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aebe0ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7fe2c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model generation\n",
    "y_test_pred=pipe.predict(test_data['Phrase']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9fae8fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, ..., 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "583b93ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"It 's a perfect show of respect to just one of those underrated professionals who deserve but rarely receive it .\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa9770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
